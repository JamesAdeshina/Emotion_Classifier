{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acbc7d2403c10b3b",
   "metadata": {},
   "source": [
    "### Emotion Analysis"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:46:51.155968Z",
     "start_time": "2024-12-19T04:46:10.877693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install emoji\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ],
   "id": "8f8920283c7ee775",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /opt/anaconda3/lib/python3.12/site-packages (2.14.0)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/apple2015/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/apple2015/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /opt/anaconda3/lib/python3.12/site-packages (3.6.0)\r\n",
      "Requirement already satisfied: absl-py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (2.1.0)\r\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.12/site-packages (from keras) (1.26.4)\r\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras) (13.3.5)\r\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.0.7)\r\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.12/site-packages (from keras) (3.11.0)\r\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.12.1)\r\n",
      "Requirement already satisfied: ml-dtypes in /opt/anaconda3/lib/python3.12/site-packages (from keras) (0.4.0)\r\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from keras) (23.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from optree->keras) (4.11.0)\r\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\r\n",
      "Requirement already satisfied: tensorflow in /opt/anaconda3/lib/python3.12/site-packages (2.17.0)\r\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\r\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\r\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\r\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.5.3)\r\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\r\n",
      "Requirement already satisfied: h5py>=3.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.11.0)\r\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (0.4.0)\r\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.3.0)\r\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (23.2)\r\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.25.3)\r\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.32.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (69.5.1)\r\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\r\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (4.11.0)\r\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.14.1)\r\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (1.62.2)\r\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (2.17.0)\r\n",
      "Requirement already satisfied: keras>=3.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorflow) (3.6.0)\r\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/anaconda3/lib/python3.12/site-packages (from h5py>=3.10.0->tensorflow) (1.26.4)\r\n",
      "Requirement already satisfied: rich in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (13.3.5)\r\n",
      "Requirement already satisfied: namex in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.0.7)\r\n",
      "Requirement already satisfied: optree in /opt/anaconda3/lib/python3.12/site-packages (from keras>=3.2.0->tensorflow) (0.12.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.2)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\r\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.4.1)\r\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.0)\r\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.3)\r\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.2.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich->keras>=3.2.0->tensorflow) (2.15.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.0)\r\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "affe3330009c94a2",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "da29198623cafa10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T04:48:11.929060Z",
     "start_time": "2024-12-19T04:48:11.875426Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# from tensorflow.keras import layers, models"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow.keras'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers, models\n\u001B[0;32m---> 23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tokenizer\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pad_sequences\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow.keras'"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "95518ec18d1eaba6",
   "metadata": {},
   "source": [
    "#### Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8f92b59e62f46f",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset from a file.\"\"\"\n",
    "    return pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357f3cf7a7ccacd1",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def explore_data(df, text_column='text'):\n",
    "    \"\"\"\n",
    "    Perform an extensive exploration of the dataset to check data cleanliness.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Pandas DataFrame\n",
    "    - text_column: Name of the column containing text data\n",
    "\n",
    "    Returns:\n",
    "    - Summary of findings\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Basic Information ---\")\n",
    "    print(f\"Dataset Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Missing Values:\\n{df.isnull().sum()}\")\n",
    "    print(f\"Duplicate Rows: {df.duplicated().sum()}\")\n",
    "\n",
    "    print(\"\\n--- Class Distribution ---\")\n",
    "    if 'label' in df.columns:\n",
    "        print(df['label'].value_counts())\n",
    "    else:\n",
    "        print(\"No 'label' column found!\")\n",
    "\n",
    "    print(\"\\n--- Text Analysis ---\")\n",
    "    # Check for empty or blank text\n",
    "    empty_texts = df[text_column].isnull().sum() + df[text_column].str.strip().eq('').sum()\n",
    "    print(f\"Empty or Blank Texts: {empty_texts}\")\n",
    "\n",
    "    # Check for punctuation\n",
    "    punctuations = df[text_column].apply(lambda x: len(re.findall(r'[^\\w\\s]', str(x))))\n",
    "    print(f\"Average Punctuation Count per Entry: {punctuations.mean():.2f}\")\n",
    "\n",
    "    # Check for emojis\n",
    "    emojis = df[text_column].apply(lambda x: len(emoji.emoji_list(str(x))))\n",
    "    print(f\"Average Emoji Count per Entry: {emojis.mean():.2f}\")\n",
    "\n",
    "    # Check for stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_word_counts = df[text_column].apply(lambda x: len([word for word in str(x).split() if word.lower() in stop_words]))\n",
    "    print(f\"Average Stop Words per Entry: {stop_word_counts.mean():.2f}\")\n",
    "\n",
    "    # Check for special characters (non-alphanumeric)\n",
    "    special_chars = df[text_column].apply(lambda x: len(re.findall(r'[^\\w\\s]', str(x))))\n",
    "    print(f\"Average Special Characters per Entry: {special_chars.mean():.2f}\")\n",
    "\n",
    "    print(\"\\n--- Recommendations ---\")\n",
    "    recommendations = []\n",
    "    if empty_texts > 0:\n",
    "        recommendations.append(f\"Remove or handle {empty_texts} empty or blank entries.\")\n",
    "    if df.duplicated().sum() > 0:\n",
    "        recommendations.append(\"Remove duplicate rows.\")\n",
    "    if emojis.mean() > 0:\n",
    "        recommendations.append(\"Consider handling emojis (e.g., replace with words or remove).\")\n",
    "    if punctuations.mean() > 0:\n",
    "        recommendations.append(\"Remove or handle punctuation marks appropriately.\")\n",
    "    if special_chars.mean() > 0:\n",
    "        recommendations.append(\"Clean special characters from text.\")\n",
    "\n",
    "    if recommendations:\n",
    "        print(\"\\n\".join(recommendations))\n",
    "    else:\n",
    "        print(\"The dataset appears clean!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155f6e315113b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop empty rows from our data\n",
    "def drop_empty_rows(df):\n",
    "    \"\"\"Drop rows with empty text values.\"\"\"\n",
    "    return df.dropna(subset=['text']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339eedc44668e730",
   "metadata": {},
   "source": [
    "#### Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bd508c16219733",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:22.034174Z",
     "start_time": "2024-12-08T18:58:22.025693Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "    \"\"\"Remove emojis from text.\"\"\"\n",
    "    return emoji.replace_emoji(text, replace=\"\")\n",
    "\n",
    "def remove_punctuation_and_symbols(text):\n",
    "    \"\"\"Remove punctuation, numbers, and special characters.\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_punctuation_and_symbols(text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def apply_preprocessing(df):\n",
    "    \"\"\"Apply preprocessing to the text column.\"\"\"\n",
    "    df['text'] = df['text'].apply(preprocess_text)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2371f0912fa9de6d",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae90cd91878a822",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:27.289929Z",
     "start_time": "2024-12-08T18:58:27.276910Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_features_Tfidf(corpus):\n",
    "    \"\"\"Convert text into numerical representations.\"\"\"\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return features, vectorizer\n",
    "\n",
    "def create_features_CountVectorizer(corpus):\n",
    "    \"\"\"Convert text into numerical representations.\"\"\"\n",
    "    vectorizer = CountVectorizer(max_features=5000)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return features, vectorizer\n",
    "\n",
    "def create_features_HashingVectorizer(corpus):\n",
    "    \"\"\"Convert text into numerical representations using HashingVectorizer.\"\"\"\n",
    "    vectorizer = HashingVectorizer(n_features=5000)\n",
    "    features = vectorizer.fit_transform(corpus)\n",
    "    return features, vectorizer\n",
    "\n",
    "def create_features_Word2Vec(corpus):\n",
    "    \"\"\"Convert text into numerical representations using Word2Vec.\"\"\"\n",
    "    tokenized_corpus = [doc.split() for doc in corpus]\n",
    "    model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    features = np.array([np.mean([model.wv[word] for word in doc if word in model.wv] or [np.zeros(100)], axis=0) for doc in tokenized_corpus])\n",
    "    return features, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed49450c558445c",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cdcfa189750968",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63d7bfe4eacfe85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:31.467768Z",
     "start_time": "2024-12-08T18:58:31.460899Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_logistic_regression(X, y):\n",
    "    \"\"\"Train and evaluate a Logistic Regression model.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"\\n--- Logistic Regression Evaluation ---\")\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfbb560f6a2a251",
   "metadata": {},
   "source": [
    "##### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a6f4e4cd8223e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:34.658590Z",
     "start_time": "2024-12-08T18:58:34.639346Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_random_forest(X, y):\n",
    "    \"\"\"Train and evaluate a Random Forest Classifier.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2, random_state=42)\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"\\n--- Random Forest Evaluation ---\")\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153c9dd7aacc9bc",
   "metadata": {},
   "source": [
    "##### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ea5462c80f349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:37.416652Z",
     "start_time": "2024-12-08T18:58:37.399382Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_svm(X, y):\n",
    "    \"\"\"Train and evaluate a Support Vector Machine (SVM).\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = SVC(probability=True)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"\\n--- SVM Evaluation ---\")\n",
    "    evaluate_model(model, X_test, y_test)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3164d5a10ba74dd7",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e290795d534f01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:39.222136Z",
     "start_time": "2024-12-08T18:58:39.199537Z"
    }
   },
   "outputs": [],
   "source": [
    "#Evaluation Function\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate the performance of a trained model.\"\"\"\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39275ac3d8f700a",
   "metadata": {},
   "source": [
    "#### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a1e03edcde6613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:58:41.926293Z",
     "start_time": "2024-12-08T18:58:41.893766Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_emotion(model, text, vectorizer):\n",
    "    \"\"\"Predict the emotion of a single input text.\"\"\"\n",
    "    processed_text = preprocess_text(text)\n",
    "    features = vectorizer.transform([processed_text])\n",
    "    return model.predict(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5271ce0040574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:29:41.901209Z",
     "start_time": "2024-12-08T19:29:28.727042Z"
    }
   },
   "outputs": [],
   "source": [
    "df = load_data('sampled_data.csv')\n",
    "explore_data(df, text_column='text')  # Adjust 'text' if your column name differs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e40409574a510c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:39:30.767097Z",
     "start_time": "2024-12-08T19:29:57.704939Z"
    }
   },
   "outputs": [],
   "source": [
    "df = apply_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332940142b1a869c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:40:51.080212Z",
     "start_time": "2024-12-08T19:40:43.447302Z"
    }
   },
   "outputs": [],
   "source": [
    "X, vectorizer = create_features(df['text'])  # 'text' is the name of your text column\n",
    "y = df['label']  # Assuming the label column is named 'label'\n",
    "\n",
    "logistic_model = train_logistic_regression(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3847ac9d9651a15a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:46:41.986155Z",
     "start_time": "2024-12-08T19:46:41.882302Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the label to emotion mapping\n",
    "label_to_emotion = {\n",
    "    0: 'sadness',\n",
    "    1: 'joy',\n",
    "    2: 'love',\n",
    "    3: 'anger',\n",
    "    4: 'fear'\n",
    "}\n",
    "\n",
    "\n",
    "# text_to_predict = \"I am so happy today!\"\n",
    "text_to_predict = \"it doesn't necessarily convey sadness or disappointment.\"\n",
    "\n",
    "# Use the trained Logistic Regression model\n",
    "predicted_emotion_logistic = predict_emotion(logistic_model, text_to_predict, vectorizer)\n",
    "print(f\"Predicted emotion (Logistic Regression): {predicted_emotion_logistic}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d84b33ec3d86e1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:48:12.455710Z",
     "start_time": "2024-12-08T19:48:11.860619Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n--- Logistic Regression Evaluation ---\")\n",
    "evaluate_model(logistic_model, X, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab80255e5eb52854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T19:52:24.128983Z",
     "start_time": "2024-12-08T19:49:24.277784Z"
    }
   },
   "outputs": [],
   "source": [
    "random_forest_model = train_random_forest(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc2dfd1049f74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-09T02:44:46.725853Z",
     "start_time": "2024-12-08T20:00:08.147381Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_model = train_svm(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523c09bbb7254635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM\n",
    "# Prepare data for LSTM\n",
    "# define model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
    "lstm_model.add(Dense(1))\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "lstm_model.fit(X, y, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85366719-8b91-4be8-b56d-d14001ec904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_predictions = lstm_model.predict(text_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612bbf550914ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_lstm, y_lstm, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27566aff4d9cd794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LSTM model\n",
    "def build_lstm_model(vocab_size, embedding_dim, input_length):\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=input_length),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c7c22424fb46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = build_lstm_model(vocab_size=20000, embedding_dim=100, input_length=100)\n",
    "lstm_model.fit(X_train_lstm, y_train_lstm, validation_split=0.2, epochs=5, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592e70a0bb56b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate LSTM\n",
    "lstm_predictions = lstm_model.predict(X_test_lstm)\n",
    "lstm_pred_classes = np.argmax(lstm_predictions, axis=1)\n",
    "print(\"\\n--- LSTM Evaluation ---\")\n",
    "print(classification_report(y_test_lstm, lstm_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bee4896737ad86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for BERT\n",
    "def prepare_data_for_bert(df, text_column='text', label_column='label'):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    inputs = tokenizer(list(df[text_column]), padding=True, truncation=True, return_tensors='pt', max_length=128)\n",
    "    labels = torch.tensor(df[label_column].values)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38cece7c2de65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs, bert_labels = prepare_data_for_bert(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30fd0f6537ed6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split for BERT\n",
    "train_size = int(0.8 * len(bert_labels))\n",
    "train_inputs = {k: v[:train_size] for k, v in bert_inputs.items()}\n",
    "train_labels = bert_labels[:train_size]\n",
    "test_inputs = {k: v[train_size:] for k, v in bert_inputs.items()}\n",
    "test_labels = bert_labels[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ac3b0e7f6a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bc77320132cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d062d41a3eed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=list(zip(train_inputs['input_ids'], train_labels)),\n",
    "    eval_dataset=list(zip(test_inputs['input_ids'], test_labels))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b5d4559aaddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5f6781920afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate BERT\n",
    "bert_outputs = bert_model(**test_inputs)\n",
    "bert_pred_classes = torch.argmax(bert_outputs.logits, axis=1)\n",
    "print(\"\\n--- BERT Evaluation ---\")\n",
    "print(classification_report(test_labels.numpy(), bert_pred_classes.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b975ba-669b-4040-962b-3bf6c1e4ed12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
